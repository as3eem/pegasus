{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hemant/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PegasusTokenizer,PegasusForConditionalGeneration\n",
    "from rouge_score import rouge_scorer\n",
    "import torch\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lr3LzFBrGuM-",
    "outputId": "927bba91-e372-4e1b-be68-a2b7e0a269c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already prepared... Importing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_path = \"AMICorpusXML/data/ami-summary/abstractive/ES2004a.abssumm.txt\"\n",
    "from pathlib import Path\n",
    "\n",
    "my_file = Path(check_path)\n",
    "if not my_file.is_file():\n",
    "    if not Path(\"AMICorpusXML/data\").is_dir():\n",
    "        ! git clone https://github.com/gcunhase/AMICorpusXML\n",
    "    ! python /content/AMICorpusXML/main_obtain_meeting2summary_data.py --summary_type abstractive\n",
    "else:\n",
    "    print(\"Data already prepared... Importing\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking same split to make easy comparision\n",
    "# data split knowledge: http://groups.inf.ed.ac.uk/ami/corpus/datasets.shtml\n",
    "# 5 x 4 = 20 \n",
    "test = \"ES2004, ES2014, IS1009, TS3003, TS3007\".split(',')\n",
    "\n",
    "val = \"ES2003, ES2011, IS1008, TS3004, TS3006\".split(',')\n",
    "train = \"\"\"ES2002, ES2005, ES2006, ES2007, ES2008, ES2009, ES2010, ES2012, ES2013, ES2015, \n",
    "            ES2016, IS1000, IS1001, IS1003, IS1004, IS1006, IS1007, TS3005, TS3008, TS3009, TS3010, \n",
    "            TS3011, TS3012\"\"\".split(\",\")\n",
    "\n",
    "\n",
    "def data_create(path):\n",
    "    story = []\n",
    "    story_directory = r'AMICorpusXML/data/ami-transcripts-stories/abstractive'\n",
    "    for filename in os.listdir(story_directory):\n",
    "        if filename.endswith(\".story\"):\n",
    "            for each in path:\n",
    "                if each.strip() in str(filename):\n",
    "                    story.append(filename)\n",
    "    summary = []\n",
    "    sum_directory = r'AMICorpusXML/data/ami-summary/abstractive'\n",
    "    for filename in os.listdir(sum_directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            for each in path:\n",
    "                if each.strip() in str(filename):\n",
    "                    summary.append(filename)\n",
    "    return [data_from_file(f\"AMICorpusXML/data/ami-transcripts-stories/abstractive/{story[i]}\", f\"AMICorpusXML/data/ami-summary/abstractive/{summary[i]}\") for i in range(len(story))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(model_name = 'google/pegasus-xsum'):\n",
    "    print(\"----------------- Preparing Model -----------------\")\n",
    "    \n",
    "    torch_device = \"cpu\" #'cuda'if torch.cuda.is_available() else 'cpu'\n",
    "    tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "    model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "    print(\"----------------- Model Prepared -----------------\")\n",
    "    return tokenizer, model, torch_device\n",
    "\n",
    "def test_model(src_text, tokenizer, model):\n",
    "#     print(\"\\nNew Testing started...\")\n",
    "    batch = tokenizer.prepare_seq2seq_batch(src_text,max_length=512).to(torch_device)\n",
    "    translated = model.generate(**batch)\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text\n",
    "\n",
    "def data_from_file(f1,f2):\n",
    "    with open(f1, 'r') as file:\n",
    "        data1 = file.read().replace('\\n', '')\n",
    "    with open(f2, 'r') as file:\n",
    "        data2 = file.read().replace('\\n', '')\n",
    "    return data1, data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TGWmiqXGJZL2",
    "outputId": "3c83dc81-6cf8-4742-e66f-486b6f059d64",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# taking same split to make easy comparision\n",
    "# data split knowledge: http://groups.inf.ed.ac.uk/ami/corpus/datasets.shtml\n",
    "# 5 x 4 = 20 \n",
    "test = \"ES2004, ES2014, IS1009, TS3003, TS3007\".split(',')\n",
    "\n",
    "val = \"ES2003, ES2011, IS1008, TS3004, TS3006\".split(',')\n",
    "train = \"\"\"ES2002, ES2005, ES2006, ES2007, ES2008, ES2009, ES2010, ES2012, ES2013, ES2015, \n",
    "            ES2016, IS1000, IS1001, IS1003, IS1004, IS1006, IS1007, TS3005, TS3008, TS3009, TS3010, \n",
    "            TS3011, TS3012\"\"\".split(\",\")\n",
    "\n",
    "story = []\n",
    "story_directory = r'AMICorpusXML/data/ami-transcripts-stories/abstractive'\n",
    "for filename in os.listdir(story_directory):\n",
    "    if filename.endswith(\".story\"):\n",
    "        for each in test:\n",
    "            if each.strip() in str(filename):\n",
    "                story.append(filename)\n",
    "     \n",
    "summary = []\n",
    "sum_directory = r'AMICorpusXML/data/ami-summary/abstractive'\n",
    "for filename in os.listdir(sum_directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        for each in test:\n",
    "            if each.strip() in str(filename):\n",
    "                summary.append(filename)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "st, sums_pred, sums_ann = [], [], []\n",
    "\n",
    "# remove later\n",
    "# flag = 0\n",
    "\n",
    "tokenizer, model, torch_device = prepare_model()\n",
    "model.eval()\n",
    "for each in tqdm(story):\n",
    "    a = each.split('.')[0]\n",
    "    story_file = story_directory+'/'+each\n",
    "    summ_file = sum_directory+'/'+a+\".abssumm.txt\"\n",
    "    story_file, summ_file = data_from_file(story_file, summ_file)\n",
    "    st.append(story_file)\n",
    "    src_text = [story_file.replace('\\n',' ')]\n",
    "    result = test_model(src_text, tokenizer, model)\n",
    "    sums_pred.append(result[0])\n",
    "    sums_ann.append(summ_file)\n",
    "\n",
    "    # flag+=1\n",
    "    # if flag >=2: break\n",
    "\n",
    "df['stories'] = st\n",
    "df['summaries'] = sums_pred\n",
    "df['Summary annotated'] = sums_ann\n",
    "df.to_csv('summary_df.csv')\n",
    "display(df.head())\n",
    "\n",
    "\n",
    "test# R1 and R2 for a sample summary \n",
    "a = df[\"stories\"][0]\n",
    "b = df[\"summaries\"][0]\n",
    "c = df[\"Summary annotated\"][0]\n",
    "\n",
    "def rogue_sc(b, a):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "    scores = scorer.score(b,a)\n",
    "    return scores\n",
    "\n",
    "scores = rogue_sc(b,a)\n",
    "print(\"Word Capture: \", scores['rouge1'][1])\n",
    "scores = rogue_sc(b,c)\n",
    "print(\"ROUGE-1 Score: \", scores['rouge1'][1])\n",
    "print(\"ROUGE-2 Score: \", scores['rouge2'][1])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "rouge1 = []\n",
    "rouge2 = []\n",
    "word_capt = []\n",
    "for index, row in df.iterrows():\n",
    "    rouge1.append(rogue_sc(row['summaries'], row['Summary annotated'])['rouge1'][1])\n",
    "    rouge2.append(rogue_sc(row['summaries'], row['Summary annotated'])['rouge2'][1])\n",
    "    word_capt.append(rogue_sc(row['summaries'], row['Summary annotated'])['rouge1'][1])\n",
    "print(\"Average Rouge1 Score: \", 100*np.average(np.array(rouge1)))\n",
    "print(\"Average Rouge2 Score: \", 100*np.average(np.array(rouge2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6W8qnUFOhQ03",
    "outputId": "6087fcb1-3ebe-425a-a5e0-e0e63a1e9a99"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_smoothed_nll_loss(lprobs, target, epsilon, ignore_index=-100):\n",
    "    \"\"\"From fairseq\"\"\"\n",
    "    if target.dim() == lprobs.dim() - 1:\n",
    "        target = target.unsqueeze(-1)\n",
    "    nll_loss = -lprobs.gather(dim=-1, index=target)\n",
    "    smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
    "    if ignore_index is not None:\n",
    "        pad_mask = target.eq(ignore_index)\n",
    "        nll_loss.masked_fill_(pad_mask, 0.0)\n",
    "        smooth_loss.masked_fill_(pad_mask, 0.0)\n",
    "    else:\n",
    "        nll_loss = nll_loss.squeeze(-1)\n",
    "        smooth_loss = smooth_loss.squeeze(-1)\n",
    "\n",
    "    nll_loss = nll_loss.sum()  # mean()? Scared to break other math.\n",
    "    smooth_loss = smooth_loss.sum()\n",
    "    eps_i = epsilon / lprobs.size(-1)\n",
    "    loss = (1.0 - epsilon) * nll_loss + eps_i * smooth_loss\n",
    "    return loss, nll_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 5\n",
    "#model.resize_token_embeddings(len(tokenizer))\n",
    "weight_decay =0.0\n",
    "learning_rate = 1e-4 \n",
    "adam_epsilon = 1e-8\n",
    "warmup_steps = 0\n",
    "t_total = (607276 // 2 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dhak chiki dhak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = data_create(test)\n",
    "train_ = data_create(train)\n",
    "val_ = data_create(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open ('../Ext_Abs/train.txt', \"r\") as f:\n",
    "    train_ = json.loads(f.read())\n",
    "with open ('../Ext_Abs/val.txt', \"r\") as f:\n",
    "    val_ = json.loads(f.read())\n",
    "with open ('../Ext_Abs/test.txt', \"r\") as f:\n",
    "    test_ = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a = []\n",
    "# for i in test_:\n",
    "#     a.append(len(i[0].split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_bart import shift_tokens_right\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'google/pegasus-xsum'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "output_dir = 'data/train/latest'\n",
    "# tokenizer = PegasusTokenizer.from_pretrained(output_dir)\n",
    "# model = PegasusForConditionalGeneration.from_pretrained(output_dir).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "# optimizer.load_state_dict(torch.load(os.path.join(output_dir, 'optimizer.pt')))\n",
    "# scheduler.load_state_dict(torch.load(os.path.join(output_dir, 'scheduler.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07096e6e64974041aa1a5865e310063c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch :0Training Loss: 683.4345703125\n",
      "batch :5Training Loss: 5419.50830078125\n",
      "batch :10Training Loss: 9996.571472167969\n",
      "batch :15Training Loss: 13984.0224609375\n",
      "batch :20Training Loss: 18024.03924560547\n",
      "batch :25Training Loss: 22027.022521972656\n",
      "batch :30Training Loss: 26687.238403320312\n",
      "batch :35Training Loss: 30294.290466308594\n",
      "batch :40Training Loss: 34886.25524902344\n",
      "batch :45Training Loss: 39493.499084472656\n",
      "\n",
      "traing loss epoch1:0.0375772831095826\n",
      "time : 0:00:09\n",
      "\n",
      "======== Epoch 2 ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9757a0748c44ffa9121dd19df9be46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch :0Training Loss: 686.2532958984375\n",
      "batch :5Training Loss: 5484.547912597656\n",
      "batch :10Training Loss: 9992.440246582031\n",
      "batch :15Training Loss: 13977.842956542969\n",
      "batch :20Training Loss: 18086.047485351562\n",
      "batch :25Training Loss: 22013.024841308594\n",
      "batch :30Training Loss: 26630.956298828125\n",
      "batch :35Training Loss: 30166.185791015625\n",
      "batch :40Training Loss: 34681.984130859375\n",
      "batch :45Training Loss: 39329.865295410156\n",
      "\n",
      "traing loss epoch2:0.0374215887963301\n",
      "time : 0:00:09\n",
      "\n",
      "======== Epoch 3 ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df89c8f948b54c67b033369bd6c758f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch :0Training Loss: 679.8965454101562\n",
      "batch :5Training Loss: 5354.5814208984375\n",
      "batch :10Training Loss: 9813.88720703125\n",
      "batch :15Training Loss: 13810.426696777344\n",
      "batch :20Training Loss: 17835.511199951172\n",
      "batch :25Training Loss: 21778.530059814453\n",
      "batch :30Training Loss: 26566.232452392578\n",
      "batch :35Training Loss: 30139.072296142578\n",
      "batch :40Training Loss: 34676.5744934082\n",
      "batch :45Training Loss: 39237.68142700195\n",
      "\n",
      "traing loss epoch3:0.037333877669141736\n",
      "time : 0:00:09\n",
      "\n",
      "======== Epoch 4 ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7386c029b74a6b807567d7f00bfbae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch :0Training Loss: 670.3478393554688\n",
      "batch :5Training Loss: 5309.8927001953125\n",
      "batch :10Training Loss: 9794.90625\n",
      "batch :15Training Loss: 13772.713623046875\n",
      "batch :20Training Loss: 17793.630157470703\n",
      "batch :25Training Loss: 21667.93832397461\n",
      "batch :30Training Loss: 26393.095489501953\n",
      "batch :35Training Loss: 29979.591766357422\n",
      "batch :40Training Loss: 34515.96969604492\n",
      "batch :45Training Loss: 39114.13345336914\n",
      "\n",
      "traing loss epoch4:0.03721632421628396\n",
      "time : 0:00:09\n",
      "\n",
      "======== Epoch 5 ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff988c5313047e1a775985ebe3891fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch :0Training Loss: 679.2166748046875\n",
      "batch :5Training Loss: 5289.9610595703125\n",
      "batch :10Training Loss: 9816.480834960938\n",
      "batch :15Training Loss: 13810.836608886719\n",
      "batch :20Training Loss: 17812.86474609375\n",
      "batch :25Training Loss: 21680.624237060547\n",
      "batch :30Training Loss: 26343.673309326172\n",
      "batch :35Training Loss: 30074.425811767578\n",
      "batch :40Training Loss: 34492.690521240234\n",
      "batch :45Training Loss: 39102.3327331543\n",
      "\n",
      "traing loss epoch5:0.03720509606444404\n",
      "time : 0:00:09\n",
      "\n",
      "======== Epoch 6 ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1a17054d2d4efd8960f6e978755710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch :0Training Loss: 661.9628295898438\n",
      "batch :5Training Loss: 5411.4520263671875\n",
      "batch :10Training Loss: 10038.666259765625\n",
      "batch :15Training Loss: 13940.25439453125\n",
      "batch :20Training Loss: 17859.56903076172\n",
      "batch :25Training Loss: 21722.38592529297\n",
      "batch :30Training Loss: 26341.318420410156\n",
      "batch :35Training Loss: 29851.283935546875\n",
      "batch :40Training Loss: 34309.06756591797\n",
      "batch :45Training Loss: 38802.180419921875\n",
      "\n",
      "traing loss epoch6:0.036919507076084046\n",
      "time : 0:00:09\n",
      "\n",
      "======== Epoch 7 ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccafd3099db4ab5ac1278978d3ea2dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch :0Training Loss: 666.0941162109375\n",
      "batch :5Training Loss: 5340.074645996094\n",
      "batch :10Training Loss: 9795.928588867188\n",
      "batch :15Training Loss: 13767.263427734375\n",
      "batch :20Training Loss: 17666.30615234375\n",
      "batch :25Training Loss: 21517.522094726562\n",
      "batch :30Training Loss: 26214.699951171875\n",
      "batch :35Training Loss: 29724.01434326172\n",
      "batch :40Training Loss: 34190.095153808594\n",
      "batch :45Training Loss: 38649.99920654297\n",
      "\n",
      "traing loss epoch7:0.03677470966203705\n",
      "time : 0:00:09\n",
      "\n",
      "======== Epoch 8 ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eda14ee50fd48eda9e4eeb44d72d5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch :0Training Loss: 670.1840209960938\n",
      "batch :5Training Loss: 5304.2584228515625\n",
      "batch :10Training Loss: 9715.911987304688\n",
      "batch :15Training Loss: 13618.990966796875\n",
      "batch :20Training Loss: 17627.981567382812\n",
      "batch :25Training Loss: 21456.589935302734\n",
      "batch :30Training Loss: 26064.718353271484\n",
      "batch :35Training Loss: 29683.265594482422\n",
      "batch :40Training Loss: 34172.37649536133\n",
      "batch :45Training Loss: 38683.600006103516\n",
      "\n",
      "traing loss epoch8:0.03680668015812033\n",
      "time : 0:00:09\n",
      "\n",
      "======== Epoch 9 ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8efed93cf912465ea4288bcd5626c784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch :0Training Loss: 675.696533203125\n",
      "batch :5Training Loss: 5272.0555419921875\n",
      "batch :10Training Loss: 9670.61669921875\n",
      "batch :15Training Loss: 13583.831787109375\n",
      "batch :20Training Loss: 17558.523651123047\n",
      "batch :25Training Loss: 21408.878875732422\n",
      "batch :30Training Loss: 25920.464569091797\n",
      "batch :35Training Loss: 29462.98403930664\n",
      "batch :40Training Loss: 33848.250579833984\n",
      "batch :45Training Loss: 38418.0163269043\n",
      "\n",
      "traing loss epoch9:0.036553982541198425\n",
      "time : 0:00:09\n",
      "\n",
      "======== Epoch 10 ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf9e4ee457d4732a106040e15d991a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch :0Training Loss: 656.5841674804688\n",
      "batch :5Training Loss: 5279.393981933594\n",
      "batch :10Training Loss: 9677.700256347656\n",
      "batch :15Training Loss: 13534.790100097656\n",
      "batch :20Training Loss: 17469.088989257812\n",
      "batch :25Training Loss: 21312.225708007812\n",
      "batch :30Training Loss: 25827.977783203125\n",
      "batch :35Training Loss: 29371.053649902344\n",
      "batch :40Training Loss: 33700.41003417969\n",
      "batch :45Training Loss: 38220.72247314453\n",
      "\n",
      "traing loss epoch10:0.036366261342257455\n",
      "time : 0:00:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data_dump/hemant/hemant/nlp/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "for i, b in enumerate(model.parameters()):\n",
    "    if i == 400: break\n",
    "    b.requires_grad = False\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "epochs = 10\n",
    "m = []\n",
    "batch_size = 2\n",
    "train_loader = [train_[i:i+batch_size] for i in range(0,len(train_),batch_size)]\n",
    "for epoc in range(epochs):\n",
    "  t0 = time.time()\n",
    "  print(\"\")\n",
    "  print('======== Epoch {} ========'.format(epoc+1))\n",
    "  model.train()\n",
    "  total_train_loss = 0\n",
    "  for i,batch in tqdm(enumerate(train_loader)):\n",
    "    title = [j[1] for j in batch]\n",
    "    body = [j[0] for j in batch]\n",
    "    \n",
    "    batch_tokens = tokenizer.prepare_seq2seq_batch(body,title,max_length=512,max_target_length=202,padding='longest').to(device)\n",
    "    m.append(batch_tokens)\n",
    "    decoder_input_ids = shift_tokens_right(batch_tokens['labels'], pad_token_id)\n",
    "    outputs = model(batch_tokens['input_ids'], attention_mask=batch_tokens['attention_mask'], decoder_input_ids=decoder_input_ids, use_cache=False)\n",
    "    lm_logits = outputs[0]\n",
    "    lprobs = torch.nn.functional.log_softmax(lm_logits, dim=-1)\n",
    "    loss, nll_loss = label_smoothed_nll_loss(\n",
    "                lprobs, batch_tokens['labels'],0.1, ignore_index=pad_token_id\n",
    "            )\n",
    "    total_train_loss += loss.item()\n",
    "  \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if i%5 == 0:\n",
    "      print(\"batch :\" + str(i)+ \"Training Loss: \" +str(total_train_loss))\n",
    "    if (i+1) % 10000 == 0:\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n",
    "        torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt')) \n",
    "  training_time = format_time(time.time() - t0)\n",
    "  avg_train_loss = total_train_loss / 1050994\n",
    "  print(\"traing loss epoch\"+str(epoc+1)+\":\"+str(avg_train_loss))\n",
    "  print(\"time : {}\".format(training_time))\n",
    "  \n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n",
    "torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db3f26a4984455794c7875622e2cb80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stories</th>\n",
       "      <th>summaries</th>\n",
       "      <th>Summary annotated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Okay. About the components design. Um for the ...</td>\n",
       "      <td>The industrial designer gave a presentation on...</td>\n",
       "      <td>The project manager opened the meeting and rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay. Everybody found his place again? Yeah? T...</td>\n",
       "      <td>The project manager opened the meeting by stat...</td>\n",
       "      <td>The project manager stated the agenda and the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Okay, is everybody ready? Mm-hmm. Um I take it...</td>\n",
       "      <td>The project manager opens the meeting by stati...</td>\n",
       "      <td>The meeting begins and the marketing expert st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Well hi everyone again. Um like before we uh I...</td>\n",
       "      <td>The marketing expert and the industrial design...</td>\n",
       "      <td>The project manager opens the meeting by going...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So is Why not save that. Do you want to replac...</td>\n",
       "      <td>The User Interface Designer and the Industrial...</td>\n",
       "      <td>The first prototype for the remote control was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             stories  \\\n",
       "0  Okay. About the components design. Um for the ...   \n",
       "1  Okay. Everybody found his place again? Yeah? T...   \n",
       "2  Okay, is everybody ready? Mm-hmm. Um I take it...   \n",
       "3  Well hi everyone again. Um like before we uh I...   \n",
       "4  So is Why not save that. Do you want to replac...   \n",
       "\n",
       "                                           summaries  \\\n",
       "0  The industrial designer gave a presentation on...   \n",
       "1  The project manager opened the meeting by stat...   \n",
       "2  The project manager opens the meeting by stati...   \n",
       "3  The marketing expert and the industrial design...   \n",
       "4  The User Interface Designer and the Industrial...   \n",
       "\n",
       "                                   Summary annotated  \n",
       "0  The project manager opened the meeting and rec...  \n",
       "1  The project manager stated the agenda and the ...  \n",
       "2  The meeting begins and the marketing expert st...  \n",
       "3  The project manager opens the meeting by going...  \n",
       "4  The first prototype for the remote control was...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Capture:  0.9642857142857143\n",
      "ROUGE-1 Score:  0.7321428571428571\n",
      "ROUGE-2 Score:  0.2909090909090909\n",
      "Average Rouge1 Score:  65.33992900951043\n",
      "Average Rouge2 Score:  22.377193575884636\n"
     ]
    }
   ],
   "source": [
    "# taking same split to make easy comparision\n",
    "# data split knowledge: http://groups.inf.ed.ac.uk/ami/corpus/datasets.shtml\n",
    "# 5 x 4 = 20 \n",
    "test = \"ES2004, ES2014, IS1009, TS3003, TS3007\".split(',')\n",
    "\n",
    "val = \"ES2003, ES2011, IS1008, TS3004, TS3006\".split(',')\n",
    "train = \"\"\"ES2002, ES2005, ES2006, ES2007, ES2008, ES2009, ES2010, ES2012, ES2013, ES2015, \n",
    "            ES2016, IS1000, IS1001, IS1003, IS1004, IS1006, IS1007, TS3005, TS3008, TS3009, TS3010, \n",
    "            TS3011, TS3012\"\"\".split(\",\")\n",
    "\n",
    "story = []\n",
    "story_directory = r'AMICorpusXML/data/ami-transcripts-stories/abstractive'\n",
    "for filename in os.listdir(story_directory):\n",
    "    if filename.endswith(\".story\"):\n",
    "        for each in test:\n",
    "            if each.strip() in str(filename):\n",
    "                story.append(filename)\n",
    "     \n",
    "summary = []\n",
    "sum_directory = r'AMICorpusXML/data/ami-summary/abstractive'\n",
    "for filename in os.listdir(sum_directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        for each in test:\n",
    "            if each.strip() in str(filename):\n",
    "                summary.append(filename)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "st, sums_pred, sums_ann = [], [], []\n",
    "\n",
    "# remove later\n",
    "# flag = 0\n",
    "\n",
    "# tokenizer, model, torch_device = prepare_model()\n",
    "model.eval()\n",
    "for each in tqdm(story):\n",
    "    a = each.split('.')[0]\n",
    "    story_file = story_directory+'/'+each\n",
    "    summ_file = sum_directory+'/'+a+\".abssumm.txt\"\n",
    "    story_file, summ_file = data_from_file(story_file, summ_file)\n",
    "    st.append(story_file)\n",
    "    src_text = [story_file.replace('\\n',' ')]\n",
    "    result = test_model(src_text, tokenizer, model)\n",
    "    sums_pred.append(result[0])\n",
    "    sums_ann.append(summ_file)\n",
    "\n",
    "    # flag+=1\n",
    "    # if flag >=2: break\n",
    "\n",
    "df['stories'] = st\n",
    "df['summaries'] = sums_pred\n",
    "df['Summary annotated'] = sums_ann\n",
    "df.to_csv('summary_df.csv')\n",
    "display(df.head())\n",
    "\n",
    "\n",
    "test# R1 and R2 for a sample summary \n",
    "a = df[\"stories\"][0]\n",
    "b = df[\"summaries\"][0]\n",
    "c = df[\"Summary annotated\"][0]\n",
    "\n",
    "def rogue_sc(b, a):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "    scores = scorer.score(b,a)\n",
    "    return scores\n",
    "\n",
    "scores = rogue_sc(b,a)\n",
    "print(\"Word Capture: \", scores['rouge1'][1])\n",
    "scores = rogue_sc(b,c)\n",
    "print(\"ROUGE-1 Score: \", scores['rouge1'][1])\n",
    "print(\"ROUGE-2 Score: \", scores['rouge2'][1])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "rouge1 = []\n",
    "rouge2 = []\n",
    "word_capt = []\n",
    "for index, row in df.iterrows():\n",
    "    rouge1.append(rogue_sc(row['summaries'], row['Summary annotated'])['rouge1'][1])\n",
    "    rouge2.append(rogue_sc(row['summaries'], row['Summary annotated'])['rouge2'][1])\n",
    "    word_capt.append(rogue_sc(row['summaries'], row['Summary annotated'])['rouge1'][1])\n",
    "print(\"Average Rouge1 Score: \", 100*np.average(np.array(rouge1)))\n",
    "print(\"Average Rouge2 Score: \", 100*np.average(np.array(rouge2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word Capture:  0.9642857142857143\n",
    "ROUGE-1 Score:  0.7321428571428571\n",
    "ROUGE-2 Score:  0.2909090909090909\n",
    "Average Rouge1 Score:  65.33992900951043\n",
    "Average Rouge2 Score:  22.377193575884636\n",
    "\n",
    "Word Capture:  0.9642857142857143\n",
    "ROUGE-1 Score:  0.6428571428571429\n",
    "ROUGE-2 Score:  0.25925925925925924\n",
    "Average Rouge1 Score:  54.62453194827271\n",
    "Average Rouge2 Score:  18.71635619801409\n",
    "\n",
    "Word Capture:  0.9642857142857143\n",
    "ROUGE-1 Score:  0.6428571428571429\n",
    "ROUGE-2 Score:  0.25925925925925924\n",
    "Average Rouge1 Score:  54.62453194827271\n",
    "Average Rouge2 Score:  18.71635619801409\n",
    "\n",
    "Word Capture:  1.0\n",
    "ROUGE-1 Score:  0.8235294117647058\n",
    "ROUGE-2 Score:  0.3125\n",
    "Average Rouge1 Score:  48.31017980314655\n",
    "Average Rouge2 Score:  14.92631477821695\n",
    "    \n",
    "    \n",
    "Word Capture:  0.9285714285714286\n",
    "ROUGE-1 Score:  0.6607142857142857\n",
    "ROUGE-2 Score:  0.2909090909090909\n",
    "Average Rouge1 Score:  63.60044823159268\n",
    "Average Rouge2 Score:  22.70891932979732\n",
    "\n",
    "Word Capture:  1.0\n",
    "ROUGE-1 Score:  0.75\n",
    "ROUGE-2 Score:  0.36363636363636365\n",
    "Average Rouge1 Score:  60.556109094109885\n",
    "Average Rouge2 Score:  23.351602084656054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PEAGASUS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
