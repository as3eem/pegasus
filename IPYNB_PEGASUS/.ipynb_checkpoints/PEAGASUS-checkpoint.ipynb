{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /media/data_dump/hemant/hemant/nlp/lib/python3.7/site-packages (4.0.0rc1)\n",
      "Requirement already satisfied: sacremoses in /media/data_dump/hemant/hemant/nlp/lib/python3.7/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /media/data_dump/hemant/hemant/nlp/lib/python3.7/site-packages (from transformers) (0.9.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /media/data_dump/hemant/hemant/nlp/lib/python3.7/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: numpy in /media/data_dump/hemant/hemant/nlp/lib/python3.7/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: packaging in /media/data_dump/hemant/hemant/nlp/lib/python3.7/site-packages (from transformers) (20.7)\n",
      "Requirement already satisfied: filelock in /media/data_dump/hemant/hemant/nlp/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /media/data_dump/hemant/hemant/nlp/lib/python3.7/site-packages (from transformers) (2.25.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /media/data_dump/hemant/hemant/nlp/lib/python3.7/site-packages (from transformers) (4.54.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /media/data_dump/hemant/hemant/nlp/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /media/data_dump/hemant/hemant/nlp/lib/python3.7/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /media/data_dump/hemant/hemant/nlp/lib/python3.7/site-packages (from requests->transformers) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /media/data_dump/hemant/hemant/nlp/lib/python3.7/site-packages (from requests->transformers) (2020.11.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /media/data_dump/hemant/hemant/nlp/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /media/data_dump/hemant/hemant/nlp/lib/python3.7/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: six in /media/data_dump/hemant/hemant/nlp/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /media/data_dump/hemant/hemant/nlp/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /media/data_dump/hemant/hemant/nlp/lib/python3.7/site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /media/data_dump/hemant/hemant/nlp/lib/python3.7/site-packages (from transformers) (4.54.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hemant/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PegasusTokenizer,PegasusForConditionalGeneration\n",
    "from rouge_score import rouge_scorer\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lr3LzFBrGuM-",
    "outputId": "927bba91-e372-4e1b-be68-a2b7e0a269c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already prepared... Importing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_path = \"AMICorpusXML/data/ami-summary/abstractive/ES2004a.abssumm.txt\"\n",
    "from pathlib import Path\n",
    "\n",
    "my_file = Path(check_path)\n",
    "if not my_file.is_file():\n",
    "    if not Path(\"AMICorpusXML/data\").is_dir():\n",
    "        ! git clone https://github.com/gcunhase/AMICorpusXML\n",
    "    ! python /content/AMICorpusXML/main_obtain_meeting2summary_data.py --summary_type abstractive\n",
    "else:\n",
    "    print(\"Data already prepared... Importing\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking same split to make easy comparision\n",
    "# data split knowledge: http://groups.inf.ed.ac.uk/ami/corpus/datasets.shtml\n",
    "# 5 x 4 = 20 \n",
    "test = \"ES2004, ES2014, IS1009, TS3003, TS3007\".split(',')\n",
    "\n",
    "val = \"ES2003, ES2011, IS1008, TS3004, TS3006\".split(',')\n",
    "train = \"\"\"ES2002, ES2005, ES2006, ES2007, ES2008, ES2009, ES2010, ES2012, ES2013, ES2015, \n",
    "            ES2016, IS1000, IS1001, IS1003, IS1004, IS1006, IS1007, TS3005, TS3008, TS3009, TS3010, \n",
    "            TS3011, TS3012\"\"\".split(\",\")\n",
    "\n",
    "\n",
    "def data_create(path):\n",
    "    story = []\n",
    "    story_directory = r'AMICorpusXML/data/ami-transcripts-stories/abstractive'\n",
    "    for filename in os.listdir(story_directory):\n",
    "        if filename.endswith(\".story\"):\n",
    "            for each in path:\n",
    "                if each.strip() in str(filename):\n",
    "                    story.append(filename)\n",
    "    summary = []\n",
    "    sum_directory = r'AMICorpusXML/data/ami-summary/abstractive'\n",
    "    for filename in os.listdir(sum_directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            for each in path:\n",
    "                if each.strip() in str(filename):\n",
    "                    summary.append(filename)\n",
    "    return [data_from_file(f\"AMICorpusXML/data/ami-transcripts-stories/abstractive/{story[i]}\", f\"AMICorpusXML/data/ami-summary/abstractive/{summary[i]}\") for i in range(len(story))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(model_name = 'google/pegasus-xsum'):\n",
    "    print(\"----------------- Preparing Model -----------------\")\n",
    "    \n",
    "    torch_device = 'cuda:1'if torch.cuda.is_available() else 'cpu'\n",
    "    tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "    model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "    print(\"----------------- Model Prepared -----------------\")\n",
    "    return tokenizer, model, torch_device\n",
    "\n",
    "def test_model(src_text, tokenizer, model):\n",
    "    print(\"\\nNew Testing started...\")\n",
    "    batch = tokenizer.prepare_seq2seq_batch(src_text, padding='max_length').to(torch_device)\n",
    "    translated = model.generate(**batch)\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text\n",
    "\n",
    "def data_from_file(f1,f2):\n",
    "    with open(f1, 'r') as file:\n",
    "        data1 = file.read().replace('\\n', '')\n",
    "    with open(f2, 'r') as file:\n",
    "        data2 = file.read().replace('\\n', '')\n",
    "    return data1, data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TGWmiqXGJZL2",
    "outputId": "3c83dc81-6cf8-4742-e66f-486b6f059d64",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# taking same split to make easy comparision\n",
    "# data split knowledge: http://groups.inf.ed.ac.uk/ami/corpus/datasets.shtml\n",
    "# 5 x 4 = 20 \n",
    "test = \"ES2004, ES2014, IS1009, TS3003, TS3007\".split(',')\n",
    "\n",
    "val = \"ES2003, ES2011, IS1008, TS3004, TS3006\".split(',')\n",
    "train = \"\"\"ES2002, ES2005, ES2006, ES2007, ES2008, ES2009, ES2010, ES2012, ES2013, ES2015, \n",
    "            ES2016, IS1000, IS1001, IS1003, IS1004, IS1006, IS1007, TS3005, TS3008, TS3009, TS3010, \n",
    "            TS3011, TS3012\"\"\".split(\",\")\n",
    "\n",
    "story = []\n",
    "story_directory = r'AMICorpusXML/data/ami-transcripts-stories/abstractive'\n",
    "for filename in os.listdir(story_directory):\n",
    "    if filename.endswith(\".story\"):\n",
    "        for each in test:\n",
    "            if each.strip() in str(filename):\n",
    "                story.append(filename)\n",
    "     \n",
    "summary = []\n",
    "sum_directory = r'AMICorpusXML/data/ami-summary/abstractive'\n",
    "for filename in os.listdir(sum_directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        for each in test:\n",
    "            if each.strip() in str(filename):\n",
    "                summary.append(filename)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "st, sums_pred, sums_ann = [], [], []\n",
    "\n",
    "# remove later\n",
    "# flag = 0\n",
    "\n",
    "tokenizer, model, torch_device = prepare_model()\n",
    "model.eval()\n",
    "for each in tqdm(story):\n",
    "    a = each.split('.')[0]\n",
    "    story_file = story_directory+'/'+each\n",
    "    summ_file = sum_directory+'/'+a+\".abssumm.txt\"\n",
    "    story_file, summ_file = data_from_file(story_file, summ_file)\n",
    "    st.append(story_file)\n",
    "    src_text = [story_file.replace('\\n',' ')]\n",
    "    result = test_model(src_text, tokenizer, model)\n",
    "    sums_pred.append(result[0])\n",
    "    sums_ann.append(summ_file)\n",
    "\n",
    "    # flag+=1\n",
    "    # if flag >=2: break\n",
    "\n",
    "df['stories'] = st\n",
    "df['summaries'] = sums_pred\n",
    "df['Summary annotated'] = sums_ann\n",
    "df.to_csv('summary_df.csv')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6W8qnUFOhQ03",
    "outputId": "6087fcb1-3ebe-425a-a5e0-e0e63a1e9a99"
   },
   "outputs": [],
   "source": [
    "test# R1 and R2 for a sample summary \n",
    "a = df[\"stories\"][0]\n",
    "b = df[\"summaries\"][0]\n",
    "c = df[\"Summary annotated\"][0]\n",
    "\n",
    "def rogue_sc(b, a):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "    scores = scorer.score(b,a)\n",
    "    return scores\n",
    "\n",
    "scores = rogue_sc(b,a)\n",
    "print(\"Word Capture: \", scores['rouge1'][1])\n",
    "scores = rogue_sc(b,c)\n",
    "print(\"ROUGE-1 Score: \", scores['rouge1'][1])\n",
    "print(\"ROUGE-2 Score: \", scores['rouge2'][1])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "rouge1 = []\n",
    "rouge2 = []\n",
    "word_capt = []\n",
    "for index, row in df.iterrows():\n",
    "    rouge1.append(rogue_sc(row['summaries'], row['Summary annotated'])['rouge1'][1])\n",
    "    rouge2.append(rogue_sc(row['summaries'], row['Summary annotated'])['rouge2'][1])\n",
    "    word_capt.append(rogue_sc(row['summaries'], row['Summary annotated'])['rouge1'][1])\n",
    "print(\"Average Rouge1 Score: \", 100*np.average(np.array(rouge1)))\n",
    "print(\"Average Rouge2 Score: \", 100*np.average(np.array(rouge2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_smoothed_nll_loss(lprobs, target, epsilon, ignore_index=-100):\n",
    "    \"\"\"From fairseq\"\"\"\n",
    "    if target.dim() == lprobs.dim() - 1:\n",
    "        target = target.unsqueeze(-1)\n",
    "    nll_loss = -lprobs.gather(dim=-1, index=target)\n",
    "    smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
    "    if ignore_index is not None:\n",
    "        pad_mask = target.eq(ignore_index)\n",
    "        nll_loss.masked_fill_(pad_mask, 0.0)\n",
    "        smooth_loss.masked_fill_(pad_mask, 0.0)\n",
    "    else:\n",
    "        nll_loss = nll_loss.squeeze(-1)\n",
    "        smooth_loss = smooth_loss.squeeze(-1)\n",
    "\n",
    "    nll_loss = nll_loss.sum()  # mean()? Scared to break other math.\n",
    "    smooth_loss = smooth_loss.sum()\n",
    "    eps_i = epsilon / lprobs.size(-1)\n",
    "    loss = (1.0 - epsilon) * nll_loss + eps_i * smooth_loss\n",
    "    return loss, nll_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 5\n",
    "#model.resize_token_embeddings(len(tokenizer))\n",
    "weight_decay =0.0\n",
    "learning_rate = 1e-4 \n",
    "adam_epsilon = 1e-8\n",
    "warmup_steps = 0\n",
    "t_total = (607276 // 2 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dhak chiki dhak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = data_create(test)\n",
    "train_ = data_create(train)\n",
    "val_ = data_create(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a = []\n",
    "for i in val_:\n",
    "    a.append(len(i[1].split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42,\n",
       " 59,\n",
       " 71,\n",
       " 79,\n",
       " 86,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 97,\n",
       " 101,\n",
       " 106,\n",
       " 107,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 118,\n",
       " 124,\n",
       " 125,\n",
       " 127,\n",
       " 127,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 131,\n",
       " 132,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 134,\n",
       " 134,\n",
       " 136,\n",
       " 136,\n",
       " 137,\n",
       " 137,\n",
       " 139,\n",
       " 141,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 146,\n",
       " 146,\n",
       " 146,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 151,\n",
       " 153,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 155,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 163,\n",
       " 164,\n",
       " 164,\n",
       " 164,\n",
       " 170,\n",
       " 170,\n",
       " 172,\n",
       " 172,\n",
       " 172,\n",
       " 174,\n",
       " 176,\n",
       " 177,\n",
       " 177,\n",
       " 181,\n",
       " 181,\n",
       " 182,\n",
       " 182,\n",
       " 183,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 185,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 189,\n",
       " 189,\n",
       " 189,\n",
       " 189,\n",
       " 191,\n",
       " 191,\n",
       " 192,\n",
       " 192,\n",
       " 192,\n",
       " 192,\n",
       " 193,\n",
       " 193,\n",
       " 193,\n",
       " 193,\n",
       " 194,\n",
       " 194,\n",
       " 194,\n",
       " 194,\n",
       " 194,\n",
       " 195,\n",
       " 195,\n",
       " 195,\n",
       " 196,\n",
       " 196,\n",
       " 196,\n",
       " 196,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 198,\n",
       " 198,\n",
       " 199,\n",
       " 199,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 201,\n",
       " 201,\n",
       " 201,\n",
       " 201,\n",
       " 201,\n",
       " 201,\n",
       " 202,\n",
       " 532]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_bart import shift_tokens_right\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'google/pegasus-xsum'\n",
    "torch_device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "output_dir = 'data/train/'\n",
    "# tokenizer = PegasusTokenizer.from_pretrained(output_dir)\n",
    "# model = PegasusForConditionalGeneration.from_pretrained(output_dir).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "# optimizer.load_state_dict(torch.load(os.path.join(output_dir, 'optimizer.pt')))\n",
    "# scheduler.load_state_dict(torch.load(os.path.join(output_dir, 'scheduler.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, b in enumerate(model.parameters()):\n",
    "    if i == 672: break\n",
    "    b.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pad_token_id = tokenizer.pad_token_id\n",
    "epochs = 5\n",
    "m = []\n",
    "for epoc in range(epochs):\n",
    "  t0 = time.time()\n",
    "  print(\"\")\n",
    "  print('======== Epoch {} ========'.format(epoc+1))\n",
    "  model.train()\n",
    "  total_train_loss = 0\n",
    "  for i,batch in enumerate(train_):\n",
    "    title = [batch[1]]\n",
    "    print(len(title[0].split()))\n",
    "    body = [batch[0]]\n",
    "    print(len(body[0].split()))\n",
    "\n",
    "    batch_tokens = tokenizer.prepare_seq2seq_batch(body,title,max_length=len(body[0].split()),max_target_length=len(title[0].split()),padding='max_length').to(device)\n",
    "    decoder_input_ids = shift_tokens_right(batch_tokens['labels'], pad_token_id)\n",
    "    outputs = model(batch_tokens['input_ids'], attention_mask=batch_tokens['attention_mask'], decoder_input_ids=decoder_input_ids, use_cache=False)\n",
    "#     lm_logits = outputs[0]\n",
    "#     lprobs = torch.nn.functional.log_softmax(lm_logits, dim=-1)\n",
    "#     loss, nll_loss = label_smoothed_nll_loss(\n",
    "#                 lprobs, batch_tokens['labels'],0.1, ignore_index=pad_token_id\n",
    "#             )\n",
    "#     total_train_loss += loss.item()\n",
    "  \n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#     optimizer.step()\n",
    "#     scheduler.step()\n",
    "#     if i%5 == 0:\n",
    "#       print(\"batch :\" + str(i)+ \"Training Loss: \" +str(total_train_loss))\n",
    "#     if (i+1) % 10000 == 0:\n",
    "#         model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "#         model_to_save.save_pretrained(output_dir)\n",
    "#         tokenizer.save_pretrained(output_dir)\n",
    "#         torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n",
    "#         torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt')) \n",
    "#   training_time = format_time(time.time() - t0)\n",
    "#   avg_train_loss = total_train_loss / 1050994\n",
    "#   print(\"traing loss epoch\"+str(epoc+1)+\":\"+str(avg_train_loss))\n",
    "#   print(\"time : {}\".format(training_time))\n",
    "  \n",
    "#   model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "#   model_to_save.save_pretrained(output_dir)\n",
    "#   tokenizer.save_pretrained(output_dir)\n",
    "#   torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n",
    "#   torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(batch_tokens['input_ids'], attention_mask=batch_tokens['attention_mask'], decoder_input_ids=decoder_input_ids, use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        # This function also supports truncation and conversion\n",
    "                        # to pytorch tensors, but we need to do padding, so we\n",
    "                        # can't use these features :( .\n",
    "                        #max_length = 128,          # Truncate all sentences.\n",
    "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_sent)\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PEAGASUS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
