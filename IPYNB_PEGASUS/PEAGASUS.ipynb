{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers==3.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hemant/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PegasusTokenizer,PegasusForConditionalGeneration\n",
    "from rouge_score import rouge_scorer\n",
    "import torch\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lr3LzFBrGuM-",
    "outputId": "927bba91-e372-4e1b-be68-a2b7e0a269c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already prepared... Importing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_path = \"AMICorpusXML/data/ami-summary/abstractive/ES2004a.abssumm.txt\"\n",
    "from pathlib import Path\n",
    "\n",
    "my_file = Path(check_path)\n",
    "if not my_file.is_file():\n",
    "    if not Path(\"AMICorpusXML/data\").is_dir():\n",
    "        ! git clone https://github.com/gcunhase/AMICorpusXML\n",
    "    ! python /content/AMICorpusXML/main_obtain_meeting2summary_data.py --summary_type abstractive\n",
    "else:\n",
    "    print(\"Data already prepared... Importing\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking same split to make easy comparision\n",
    "# data split knowledge: http://groups.inf.ed.ac.uk/ami/corpus/datasets.shtml\n",
    "# 5 x 4 = 20 \n",
    "test = \"ES2004, ES2014, IS1009, TS3003, TS3007\".split(',')\n",
    "\n",
    "val = \"ES2003, ES2011, IS1008, TS3004, TS3006\".split(',')\n",
    "train = \"\"\"ES2002, ES2005, ES2006, ES2007, ES2008, ES2009, ES2010, ES2012, ES2013, ES2015, \n",
    "            ES2016, IS1000, IS1001, IS1003, IS1004, IS1006, IS1007, TS3005, TS3008, TS3009, TS3010, \n",
    "            TS3011, TS3012\"\"\".split(\",\")\n",
    "\n",
    "\n",
    "def data_create(path):\n",
    "    story = []\n",
    "    story_directory = r'AMICorpusXML/data/ami-transcripts-stories/abstractive'\n",
    "    for filename in os.listdir(story_directory):\n",
    "        if filename.endswith(\".story\"):\n",
    "            for each in path:\n",
    "                if each.strip() in str(filename):\n",
    "                    story.append(filename)\n",
    "    summary = []\n",
    "    sum_directory = r'AMICorpusXML/data/ami-summary/abstractive'\n",
    "    for filename in os.listdir(sum_directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            for each in path:\n",
    "                if each.strip() in str(filename):\n",
    "                    summary.append(filename)\n",
    "    return [data_from_file(f\"AMICorpusXML/data/ami-transcripts-stories/abstractive/{story[i]}\", f\"AMICorpusXML/data/ami-summary/abstractive/{summary[i]}\") for i in range(len(story))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(model_name = 'google/pegasus-xsum'):\n",
    "    print(\"----------------- Preparing Model -----------------\")\n",
    "    \n",
    "    torch_device = \"cpu\" #'cuda'if torch.cuda.is_available() else 'cpu'\n",
    "    tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "    model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "    print(\"----------------- Model Prepared -----------------\")\n",
    "    return tokenizer, model, torch_device\n",
    "\n",
    "def test_model(src_text, tokenizer, model):\n",
    "#     print(\"\\nNew Testing started...\")\n",
    "    batch = tokenizer.prepare_seq2seq_batch(src_text,max_length=512).to(torch_device)\n",
    "    translated = model.generate(**batch)\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text\n",
    "\n",
    "def data_from_file(f1,f2):\n",
    "    with open(f1, 'r') as file:\n",
    "        data1 = file.read().replace('\\n', '')\n",
    "    with open(f2, 'r') as file:\n",
    "        data2 = file.read().replace('\\n', '')\n",
    "    return data1, data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TGWmiqXGJZL2",
    "outputId": "3c83dc81-6cf8-4742-e66f-486b6f059d64",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Preparing Model -----------------\n",
      "----------------- Model Prepared -----------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93184fcbb8c403aa0fae705f6a9a954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stories</th>\n",
       "      <th>summaries</th>\n",
       "      <th>Summary annotated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Okay. About the components design. Um for the ...</td>\n",
       "      <td>Here's what we know so far about Apple's docki...</td>\n",
       "      <td>The project manager opened the meeting and rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay. Everybody found his place again? Yeah? T...</td>\n",
       "      <td>Here's the full meeting transcript:</td>\n",
       "      <td>The project manager stated the agenda and the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Okay, is everybody ready? Mm-hmm. Um I take it...</td>\n",
       "      <td>Mm-hmm.</td>\n",
       "      <td>The meeting begins and the marketing expert st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Well hi everyone again. Um like before we uh I...</td>\n",
       "      <td>This is the meeting where we discuss the desig...</td>\n",
       "      <td>The project manager opens the meeting by going...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So is Why not save that. Do you want to replac...</td>\n",
       "      <td>So I've tried to transfer it to My Documents, ...</td>\n",
       "      <td>The first prototype for the remote control was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             stories  \\\n",
       "0  Okay. About the components design. Um for the ...   \n",
       "1  Okay. Everybody found his place again? Yeah? T...   \n",
       "2  Okay, is everybody ready? Mm-hmm. Um I take it...   \n",
       "3  Well hi everyone again. Um like before we uh I...   \n",
       "4  So is Why not save that. Do you want to replac...   \n",
       "\n",
       "                                           summaries  \\\n",
       "0  Here's what we know so far about Apple's docki...   \n",
       "1                Here's the full meeting transcript:   \n",
       "2                                            Mm-hmm.   \n",
       "3  This is the meeting where we discuss the desig...   \n",
       "4  So I've tried to transfer it to My Documents, ...   \n",
       "\n",
       "                                   Summary annotated  \n",
       "0  The project manager opened the meeting and rec...  \n",
       "1  The project manager stated the agenda and the ...  \n",
       "2  The meeting begins and the marketing expert st...  \n",
       "3  The project manager opens the meeting by going...  \n",
       "4  The first prototype for the remote control was...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Capture:  0.9166666666666666\n",
      "ROUGE-1 Score:  0.0\n",
      "ROUGE-2 Score:  0.0\n",
      "Average Rouge1 Score:  35.14517243762526\n",
      "Average Rouge2 Score:  9.775370970492922\n"
     ]
    }
   ],
   "source": [
    "# taking same split to make easy comparision\n",
    "# data split knowledge: http://groups.inf.ed.ac.uk/ami/corpus/datasets.shtml\n",
    "# 5 x 4 = 20 \n",
    "test = \"ES2004, ES2014, IS1009, TS3003, TS3007\".split(',')\n",
    "\n",
    "val = \"ES2003, ES2011, IS1008, TS3004, TS3006\".split(',')\n",
    "train = \"\"\"ES2002, ES2005, ES2006, ES2007, ES2008, ES2009, ES2010, ES2012, ES2013, ES2015, \n",
    "            ES2016, IS1000, IS1001, IS1003, IS1004, IS1006, IS1007, TS3005, TS3008, TS3009, TS3010, \n",
    "            TS3011, TS3012\"\"\".split(\",\")\n",
    "\n",
    "story = []\n",
    "story_directory = r'AMICorpusXML/data/ami-transcripts-stories/abstractive'\n",
    "for filename in os.listdir(story_directory):\n",
    "    if filename.endswith(\".story\"):\n",
    "        for each in test:\n",
    "            if each.strip() in str(filename):\n",
    "                story.append(filename)\n",
    "     \n",
    "summary = []\n",
    "sum_directory = r'AMICorpusXML/data/ami-summary/abstractive'\n",
    "for filename in os.listdir(sum_directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        for each in test:\n",
    "            if each.strip() in str(filename):\n",
    "                summary.append(filename)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "st, sums_pred, sums_ann = [], [], []\n",
    "\n",
    "# remove later\n",
    "# flag = 0\n",
    "\n",
    "# tokenizer, model, torch_device = prepare_model()\n",
    "model.eval()\n",
    "for each in tqdm(story):\n",
    "    a = each.split('.')[0]\n",
    "    story_file = story_directory+'/'+each\n",
    "    summ_file = sum_directory+'/'+a+\".abssumm.txt\"\n",
    "    story_file, summ_file = data_from_file(story_file, summ_file)\n",
    "    st.append(story_file)\n",
    "    src_text = [story_file.replace('\\n',' ')]\n",
    "    result = test_model(src_text, tokenizer, model)\n",
    "    sums_pred.append(result[0])\n",
    "    sums_ann.append(summ_file)\n",
    "\n",
    "    # flag+=1\n",
    "    # if flag >=2: break\n",
    "\n",
    "df['stories'] = st\n",
    "df['summaries'] = sums_pred\n",
    "df['Summary annotated'] = sums_ann\n",
    "df.to_csv('summary_df.csv')\n",
    "display(df.head())\n",
    "\n",
    "\n",
    "test# R1 and R2 for a sample summary \n",
    "a = df[\"stories\"][0]\n",
    "b = df[\"summaries\"][0]\n",
    "c = df[\"Summary annotated\"][0]\n",
    "\n",
    "def rogue_sc(b, a):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "    scores = scorer.score(b,a)\n",
    "    return scores\n",
    "\n",
    "scores = rogue_sc(b,a)\n",
    "print(\"Word Capture: \", scores['rouge1'][1])\n",
    "scores = rogue_sc(b,c)\n",
    "print(\"ROUGE-1 Score: \", scores['rouge1'][1])\n",
    "print(\"ROUGE-2 Score: \", scores['rouge2'][1])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "rouge1 = []\n",
    "rouge2 = []\n",
    "word_capt = []\n",
    "for index, row in df.iterrows():\n",
    "    rouge1.append(rogue_sc(row['summaries'], row['Summary annotated'])['rouge1'][1])\n",
    "    rouge2.append(rogue_sc(row['summaries'], row['Summary annotated'])['rouge2'][1])\n",
    "    word_capt.append(rogue_sc(row['summaries'], row['Summary annotated'])['rouge1'][1])\n",
    "print(\"Average Rouge1 Score: \", 100*np.average(np.array(rouge1)))\n",
    "print(\"Average Rouge2 Score: \", 100*np.average(np.array(rouge2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6W8qnUFOhQ03",
    "outputId": "6087fcb1-3ebe-425a-a5e0-e0e63a1e9a99"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_smoothed_nll_loss(lprobs, target, epsilon, ignore_index=-100):\n",
    "    \"\"\"From fairseq\"\"\"\n",
    "    if target.dim() == lprobs.dim() - 1:\n",
    "        target = target.unsqueeze(-1)\n",
    "    nll_loss = -lprobs.gather(dim=-1, index=target)\n",
    "    smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
    "    if ignore_index is not None:\n",
    "        pad_mask = target.eq(ignore_index)\n",
    "        nll_loss.masked_fill_(pad_mask, 0.0)\n",
    "        smooth_loss.masked_fill_(pad_mask, 0.0)\n",
    "    else:\n",
    "        nll_loss = nll_loss.squeeze(-1)\n",
    "        smooth_loss = smooth_loss.squeeze(-1)\n",
    "\n",
    "    nll_loss = nll_loss.sum()  # mean()? Scared to break other math.\n",
    "    smooth_loss = smooth_loss.sum()\n",
    "    eps_i = epsilon / lprobs.size(-1)\n",
    "    loss = (1.0 - epsilon) * nll_loss + eps_i * smooth_loss\n",
    "    return loss, nll_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 5\n",
    "#model.resize_token_embeddings(len(tokenizer))\n",
    "weight_decay =0.0\n",
    "learning_rate = 1e-4 \n",
    "adam_epsilon = 1e-8\n",
    "warmup_steps = 0\n",
    "t_total = (607276 // 2 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dhak chiki dhak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = data_create(test)\n",
    "train_ = data_create(train)\n",
    "val_ = data_create(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open ('../Ext_Abs/train.txt', \"r\") as f:\n",
    "    train_ = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a = []\n",
    "# for i in test_:\n",
    "#     a.append(len(i[0].split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_bart import shift_tokens_right\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'google/pegasus-xsum'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "output_dir = 'data/train/'\n",
    "# tokenizer = PegasusTokenizer.from_pretrained(output_dir)\n",
    "# model = PegasusForConditionalGeneration.from_pretrained(output_dir).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "# optimizer.load_state_dict(torch.load(os.path.join(output_dir, 'optimizer.pt')))\n",
    "# scheduler.load_state_dict(torch.load(os.path.join(output_dir, 'scheduler.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = [train_[:] for i in range(len(train_))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df17483027c346cbb222302a92f4460c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch :0Training Loss: 3382.04150390625\n",
      "batch :5Training Loss: 21117.46435546875\n",
      "batch :10Training Loss: 37073.857177734375\n",
      "batch :15Training Loss: 53470.7646484375\n",
      "\n",
      "traing loss epoch1:0.05916025193694612\n",
      "time : 0:00:08\n",
      "\n",
      "======== Epoch 2 ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data_dump/hemant/hemant/nlp/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5ef7dd3ec84534a72c35577db9ec2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch :0Training Loss: 3295.244384765625\n",
      "batch :5Training Loss: 21043.076904296875\n",
      "batch :10Training Loss: 36996.321044921875\n",
      "batch :15Training Loss: 53402.813232421875\n",
      "\n",
      "traing loss epoch2:0.05915286147231514\n",
      "time : 0:00:08\n",
      "\n",
      "======== Epoch 3 ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18d0e500bba42ae8f70dd10e26f3a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch :0Training Loss: 3269.593017578125\n",
      "batch :5Training Loss: 20736.5830078125\n",
      "batch :10Training Loss: 36491.4619140625\n",
      "batch :15Training Loss: 52623.07177734375\n",
      "\n",
      "traing loss epoch3:0.05816053125399919\n",
      "time : 0:00:08\n",
      "\n",
      "======== Epoch 4 ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f01c8618dd9422d8d089bb6ca9e7261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch :0Training Loss: 3239.453857421875\n",
      "batch :5Training Loss: 20798.3994140625\n",
      "batch :10Training Loss: 36493.6494140625\n",
      "batch :15Training Loss: 52407.1533203125\n",
      "\n",
      "traing loss epoch4:0.057997312790357566\n",
      "time : 0:00:08\n",
      "\n",
      "======== Epoch 5 ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4548980ba684c86a78498152fd1339b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch :0Training Loss: 3214.54931640625\n",
      "batch :5Training Loss: 20718.232177734375\n",
      "batch :10Training Loss: 36391.16845703125\n",
      "batch :15Training Loss: 52411.671630859375\n",
      "\n",
      "traing loss epoch5:0.058038490675255995\n",
      "time : 0:00:08\n"
     ]
    }
   ],
   "source": [
    "for i, b in enumerate(model.parameters()):\n",
    "    if i == 400: break\n",
    "    b.requires_grad = False\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "epochs = 5\n",
    "m = []\n",
    "batch_size = 5\n",
    "train_loader = [train_[i:i+batch_size] for i in range(0,len(train_),batch_size)]\n",
    "for epoc in range(epochs):\n",
    "  t0 = time.time()\n",
    "  print(\"\")\n",
    "  print('======== Epoch {} ========'.format(epoc+1))\n",
    "  model.train()\n",
    "  total_train_loss = 0\n",
    "  for i,batch in tqdm(enumerate(train_loader)):\n",
    "    title = [j[1] for j in batch]\n",
    "    body = [j[0] for j in batch]\n",
    "        \n",
    "#     title = [batch[1]]\n",
    "#     body = [batch[0]]\n",
    "\n",
    "    batch_tokens = tokenizer.prepare_seq2seq_batch(body,title,max_length=512,max_target_length=202,padding='longest').to(device)\n",
    "#     print(batch_tokens['input_ids'].shape, len(body[0].split()))\n",
    "    m.append(batch_tokens)\n",
    "    decoder_input_ids = shift_tokens_right(batch_tokens['labels'], pad_token_id)\n",
    "    outputs = model(batch_tokens['input_ids'], attention_mask=batch_tokens['attention_mask'], decoder_input_ids=decoder_input_ids, use_cache=False)\n",
    "    lm_logits = outputs[0]\n",
    "    lprobs = torch.nn.functional.log_softmax(lm_logits, dim=-1)\n",
    "    loss, nll_loss = label_smoothed_nll_loss(\n",
    "                lprobs, batch_tokens['labels'],0.1, ignore_index=pad_token_id\n",
    "            )\n",
    "    total_train_loss += loss.item()\n",
    "  \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if i%5 == 0:\n",
    "      print(\"batch :\" + str(i)+ \"Training Loss: \" +str(total_train_loss))\n",
    "    if (i+1) % 10000 == 0:\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n",
    "        torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt')) \n",
    "  training_time = format_time(time.time() - t0)\n",
    "  avg_train_loss = total_train_loss / 1050994\n",
    "  print(\"traing loss epoch\"+str(epoc+1)+\":\"+str(avg_train_loss))\n",
    "  print(\"time : {}\".format(training_time))\n",
    "  \n",
    "  model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "  model_to_save.save_pretrained(output_dir)\n",
    "  tokenizer.save_pretrained(output_dir)\n",
    "  torch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\n",
    "  torch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918042b8922d4c5e892ccf3bb74055ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n",
      "New Testing started...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# taking same split to make easy comparision\n",
    "# data split knowledge: http://groups.inf.ed.ac.uk/ami/corpus/datasets.shtml\n",
    "# 5 x 4 = 20 \n",
    "test = \"ES2004, ES2014, IS1009, TS3003, TS3007\".split(',')\n",
    "\n",
    "val = \"ES2003, ES2011, IS1008, TS3004, TS3006\".split(',')\n",
    "train = \"\"\"ES2002, ES2005, ES2006, ES2007, ES2008, ES2009, ES2010, ES2012, ES2013, ES2015, \n",
    "            ES2016, IS1000, IS1001, IS1003, IS1004, IS1006, IS1007, TS3005, TS3008, TS3009, TS3010, \n",
    "            TS3011, TS3012\"\"\".split(\",\")\n",
    "\n",
    "story = []\n",
    "story_directory = r'AMICorpusXML/data/ami-transcripts-stories/abstractive'\n",
    "for filename in os.listdir(story_directory):\n",
    "    if filename.endswith(\".story\"):\n",
    "        for each in test:\n",
    "            if each.strip() in str(filename):\n",
    "                story.append(filename)\n",
    "     \n",
    "summary = []\n",
    "sum_directory = r'AMICorpusXML/data/ami-summary/abstractive'\n",
    "for filename in os.listdir(sum_directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        for each in test:\n",
    "            if each.strip() in str(filename):\n",
    "                summary.append(filename)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "st, sums_pred, sums_ann = [], [], []\n",
    "\n",
    "# remove later\n",
    "# flag = 0\n",
    "# torch_device = \"cpu\"\n",
    "model.eval()\n",
    "\n",
    "def test_model(src_text, tokenizer, model):\n",
    "    print(\"\\nNew Testing started...\")\n",
    "    batch = tokenizer.prepare_seq2seq_batch(src_text,max_length=512).to(torch_device)\n",
    "    translated = model.generate(**batch)\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text\n",
    "\n",
    "for each in tqdm(story):\n",
    "    a = each.split('.')[0]\n",
    "    story_file = story_directory+'/'+each\n",
    "    summ_file = sum_directory+'/'+a+\".abssumm.txt\"\n",
    "    story_file, summ_file = data_from_file(story_file, summ_file)\n",
    "    st.append(story_file)\n",
    "    src_text = [story_file.replace('\\n',' ')]\n",
    "    result = test_model(src_text, tokenizer, model)\n",
    "    sums_pred.append(result[0])\n",
    "    sums_ann.append(summ_file)\n",
    "\n",
    "    # flag+=1\n",
    "    # if flag >=2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stories</th>\n",
       "      <th>summaries</th>\n",
       "      <th>Summary annotated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Okay. About the components design. Um for the ...</td>\n",
       "      <td>The project manager opens the meeting by expla...</td>\n",
       "      <td>The project manager opened the meeting and rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay. Everybody found his place again? Yeah? T...</td>\n",
       "      <td>The project manager opens the meeting by stati...</td>\n",
       "      <td>The project manager stated the agenda and the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Okay, is everybody ready? Mm-hmm. Um I take it...</td>\n",
       "      <td>The project manager opens the meeting by stati...</td>\n",
       "      <td>The meeting begins and the marketing expert st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Well hi everyone again. Um like before we uh I...</td>\n",
       "      <td>The marketing expert and industrial designer p...</td>\n",
       "      <td>The project manager opens the meeting by going...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So is Why not save that. Do you want to replac...</td>\n",
       "      <td>The project manager opens the meeting by stati...</td>\n",
       "      <td>The first prototype for the remote control was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             stories  \\\n",
       "0  Okay. About the components design. Um for the ...   \n",
       "1  Okay. Everybody found his place again? Yeah? T...   \n",
       "2  Okay, is everybody ready? Mm-hmm. Um I take it...   \n",
       "3  Well hi everyone again. Um like before we uh I...   \n",
       "4  So is Why not save that. Do you want to replac...   \n",
       "\n",
       "                                           summaries  \\\n",
       "0  The project manager opens the meeting by expla...   \n",
       "1  The project manager opens the meeting by stati...   \n",
       "2  The project manager opens the meeting by stati...   \n",
       "3  The marketing expert and industrial designer p...   \n",
       "4  The project manager opens the meeting by stati...   \n",
       "\n",
       "                                   Summary annotated  \n",
       "0  The project manager opened the meeting and rec...  \n",
       "1  The project manager stated the agenda and the ...  \n",
       "2  The meeting begins and the marketing expert st...  \n",
       "3  The project manager opens the meeting by going...  \n",
       "4  The first prototype for the remote control was...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Capture:  0.9473684210526315\n",
      "ROUGE-1 Score:  0.6491228070175439\n",
      "ROUGE-2 Score:  0.32142857142857145\n",
      "Average Rouge1 Score:  54.39812172079005\n",
      "Average Rouge2 Score:  18.367252485579392\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df['stories'] = st\n",
    "df['summaries'] = sums_pred\n",
    "df['Summary annotated'] = sums_ann\n",
    "df.to_csv('summary_df.csv')\n",
    "display(df.head())\n",
    "\n",
    "\n",
    "test# R1 and R2 for a sample summary \n",
    "a = df[\"stories\"][0]\n",
    "b = df[\"summaries\"][0]\n",
    "c = df[\"Summary annotated\"][0]\n",
    "\n",
    "def rogue_sc(b, a):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "    scores = scorer.score(b,a)\n",
    "    return scores\n",
    "\n",
    "scores = rogue_sc(b,a)\n",
    "print(\"Word Capture: \", scores['rouge1'][1])\n",
    "scores = rogue_sc(b,c)\n",
    "print(\"ROUGE-1 Score: \", scores['rouge1'][1])\n",
    "print(\"ROUGE-2 Score: \", scores['rouge2'][1])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "rouge1 = []\n",
    "rouge2 = []\n",
    "word_capt = []\n",
    "for index, row in df.iterrows():\n",
    "    rouge1.append(rogue_sc(row['summaries'], row['Summary annotated'])['rouge1'][1])\n",
    "    rouge2.append(rogue_sc(row['summaries'], row['Summary annotated'])['rouge2'][1])\n",
    "    word_capt.append(rogue_sc(row['summaries'], row['Summary annotated'])['rouge1'][1])\n",
    "print(\"Average Rouge1 Score: \", 100*np.average(np.array(rouge1)))\n",
    "print(\"Average Rouge2 Score: \", 100*np.average(np.array(rouge2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Capture:  0.9310344827586207\n",
    "# ROUGE-1 Score:  0.5344827586206896\n",
    "# ROUGE-2 Score:  0.24561403508771928\n",
    "# Average Rouge1 Score:  53.47327685825854\n",
    "# Average Rouge2 Score:  16.957205303947593"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PEAGASUS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
